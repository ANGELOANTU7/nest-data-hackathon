{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 2,
=======
   "execution_count": 7,
>>>>>>> Stashed changes
   "id": "d74e51ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 3,
=======
   "execution_count": null,
   "id": "b08f4531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c25d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4833fb98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5433087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e7a10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1bd264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> Stashed changes
   "id": "7400845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client =boto3.client('s3',endpoint_url=\"https://s3a.us-east-1.amazonaws.com\")\n",
    "s3_bucket_name='hackathon2023'\n",
    "s3 = boto3.resource('s3',\n",
    "                    aws_access_key_id= 'AKIA3AEXDSNEGXQERCGG',\n",
    "                    aws_secret_access_key='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')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 4,
   "id": "28bf0fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket.objectsCollection(s3.Bucket(name='hackathon2023'), s3.ObjectSummary)\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (RequestTimeTooSkewed) when calling the ListObjects operation: The difference between the request time and the current time is too large.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(my_bucket\u001b[38;5;241m.\u001b[39mobjects\u001b[38;5;241m.\u001b[39mfilter(Prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      3\u001b[0m bucket_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m my_bucket\u001b[38;5;241m.\u001b[39mobjects\u001b[38;5;241m.\u001b[39mfilter(Prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      6\u001b[0m     file_name\u001b[38;5;241m=\u001b[39mfile\u001b[38;5;241m.\u001b[39mkey\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m!=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\boto3\\resources\\collection.py:81\u001b[0m, in \u001b[0;36mResourceCollection.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlimit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     80\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpages():\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m page:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\boto3\\resources\\collection.py:171\u001b[0m, in \u001b[0;36mResourceCollection.pages\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# Now that we have a page iterator or single page of results\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# we start processing and yielding individual items.\u001b[39;00m\n\u001b[0;32m    170\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pages:\n\u001b[0;32m    172\u001b[0m     page_items \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent, params, page):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\botocore\\paginate.py:269\u001b[0m, in \u001b[0;36mPageIterator.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inject_starting_params(current_kwargs)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(current_kwargs)\n\u001b[0;32m    270\u001b[0m     parsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_parsed_response(response)\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m first_request:\n\u001b[0;32m    272\u001b[0m         \u001b[38;5;66;03m# The first request is handled differently.  We could\u001b[39;00m\n\u001b[0;32m    273\u001b[0m         \u001b[38;5;66;03m# possibly have a resume/starting token that tells us where\u001b[39;00m\n\u001b[0;32m    274\u001b[0m         \u001b[38;5;66;03m# to index into the retrieved page.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\botocore\\paginate.py:357\u001b[0m, in \u001b[0;36mPageIterator._make_request\u001b[1;34m(self, current_kwargs)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, current_kwargs):\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcurrent_kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\botocore\\client.py:530\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    528\u001b[0m     )\n\u001b[0;32m    529\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[1;32m--> 530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\botocore\\client.py:960\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m    958\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m parsed_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    959\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[1;32m--> 960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[1;31mClientError\u001b[0m: An error occurred (RequestTimeTooSkewed) when calling the ListObjects operation: The difference between the request time and the current time is too large."
     ]
    }
   ],
=======
   "execution_count": null,
   "id": "28bf0fce",
   "metadata": {},
   "outputs": [],
>>>>>>> Stashed changes
   "source": [
    "my_bucket=s3.Bucket(s3_bucket_name)\n",
    "print(my_bucket.objects.filter(Prefix='data'))\n",
    "bucket_list = []\n",
    "for file in my_bucket.objects.filter(Prefix = 'data'):\n",
    "    \n",
    "    file_name=file.key\n",
    "    if file_name.find(\".csv\")!=-1:\n",
    "        bucket_list.append(file.key)\n",
    "length_bucket_list=print(len(bucket_list))\n",
    "print(bucket_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 5,
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "id": "1def12b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if sys.version_info[0] < 3:\n",
    "    from StringIO import StringIO  # Python 2.x\n",
    "else:\n",
    "    from io import StringIO  # Python 3.x\n",
    "    \n",
    "df = []   # Initializing empty list of dataframes\n",
    "seq=[\"|\",\",\",\"\\t\",\",\"]\n",
    "i=0\n",
    "for file in bucket_list:\n",
    "    obj = s3.Object(s3_bucket_name,file)\n",
    "    data=obj.get()['Body'].read()\n",
    "    df.append(pd.read_csv(io.BytesIO(data), delimiter=seq[i],header=0, low_memory=False))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 6,
   "id": "be25fa42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "id": "be25fa42",
   "metadata": {},
   "outputs": [],
>>>>>>> Stashed changes
=======
   "execution_count": null,
   "id": "be25fa42",
   "metadata": {},
   "outputs": [],
>>>>>>> Stashed changes
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 7,
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "id": "c401441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PlansShiftWise.csv\n",
    "df[0]\n",
    "df[0].to_csv('../LAKEHOUSE_SILVER_LAYER/PlantShift.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 8,
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "id": "983c0970",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Result.csv\n",
    "\n",
    "\n",
    "    \n",
    "result_schema=['Id','Status','BoardId','BatchId','WorkOrderId','RoutingStageId','RoutingStageName','Operator','Deviation','InspectionDate','LastModifiedDate','ReInspectionNeeded','PreviouslySannedBoards','RoutingStatus','CavityID','SubWorkCenter','StationCode','StationName','TrayId','AssetSubNodeId','CollectionId','Company','Division']\n",
    "result_1=[]\n",
    "for col in df[1].columns:\n",
    "    result_1.append(col)\n",
    "print(result_1)\n",
    "\n",
    "\n",
    "dicts = {}\n",
    "keys = result_1\n",
    "values = result_schema\n",
    "j=0\n",
    "for i in keys:\n",
    "        dicts[i] = result_schema[j]\n",
    "        j=j+1\n",
    "print(dicts)\n",
    "\n",
    "df[1].rename(columns=dicts,\n",
    "          inplace=True)\n",
    "\n",
    "\n",
    "df[1].loc[-1] = result_1  # adding a row\n",
    "df[1].index = df[1].index + 1  # shifting index\n",
    "df[1].sort_index(inplace=True) \n",
    " \n",
    "# call rename () method\n",
    "\n",
    "df[1]\n",
    "df[1].to_csv('../LAKEHOUSE_SILVER_LAYER/result.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 9,
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "id": "d5bc4b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#workorder.csv\n",
    "\n",
    "\n",
    "    \n",
    "raw_workorder='Id | ItemId | LineNo | Description | Quantity | Started | StartDate | EndDate | EcnNo | EcnQunatity | EcnStatus | ProductRevision | PlannedStartDate | PlannedEndDate | Isblocked | BlockedDate | BlockedBy | BatchProceedStatus | WorkOrderClosureStatus | ShortClosedQuantity | CreationDate | DysonPONumber | CustomerSKUNumber | RoutingVersionId | RoutingHeaderId | ERPClosureStatus | FeederReloadLockRequired | MSDLockRequired | UnitPrice | AllowCustomerRefNoRepetition | Company | Division'\n",
    "x=raw_workorder.split(\" | \");\n",
    "workorder_schema=[]\n",
    "for i in x:\n",
    "    workorder_schema.append(i)\n",
    "    \n",
    "print(workorder_schema)\n",
    "    \n",
    "\n",
    "workorder_1=[]\n",
    "for col in df[2].columns:\n",
    "    workorder_1.append(col)\n",
    "print(workorder_1)\n",
    "\n",
    "\n",
    "dicts = {}\n",
    "keys = workorder_1\n",
    "values = workorder_schema\n",
    "j=0\n",
    "for i in keys:\n",
    "        dicts[i] = workorder_schema[j]\n",
    "        j=j+1\n",
    "print(dicts)\n",
    "\n",
    "df[2].rename(columns=dicts,\n",
    "          inplace=True)\n",
    "\n",
    "\n",
    "df[2].loc[-1] = workorder_1  # adding a row\n",
    "df[2].index = df[2].index + 1  # shifting index\n",
    "df[2].sort_index(inplace=True) \n",
    " \n",
    "# call rename () method\n",
    "\n",
    "df[2]\n",
    "df[2].to_csv('../LAKEHOUSE_SILVER_LAYER/workorder.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "execution_count": 10,
=======
   "execution_count": null,
>>>>>>> Stashed changes
=======
   "execution_count": null,
>>>>>>> Stashed changes
   "id": "fb539dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Warehouse.csv\n",
    "\n",
    "raw_warehouse='Entry No_ | Journal Batch Name | Line No_ | Registering Date | Location Code | Zone Code | Bin Code | Description | Item No_ | Quantity | Qty_ (Base) | Source Type | Source Subtype | Source No_ | Source Line No_ | Source Subline No_ | Source Document | Source Code | Reason Code | No_ Series | Bin Type Code | Cubage | Weight | Journal Template Name | Whse_ Document No_ | Whse_ Document Type | Whse_ Document Line No_ | Entry Type | Reference Document | Reference No_ | User ID | Variant Code | Qty_ per Unit of Measure | Unit of Measure Code | Serial No_ | Lot No_ | Warranty Date | Expiration Date | Phys Invt Counting Period Code | Phys Invt Counting Period Type | Dedicated | Company | Division'\n",
    "x=raw_warehouse.split(\" | \");\n",
    "warehouse_schema=[]\n",
    "for i in x:\n",
    "    warehouse_schema.append(i)\n",
    "    \n",
    "print(warehouse_schema)\n",
    "    \n",
    "\n",
    "warehouse_1=[]\n",
    "for col in df[3].columns:\n",
    "    warehouse_1.append(col)\n",
    "print(warehouse_1)\n",
    "\n",
    "\n",
    "dicts = {}\n",
    "keys = warehouse_1\n",
    "values = warehouse_schema\n",
    "j=0\n",
    "for i in keys:\n",
    "        dicts[i] = warehouse_schema[j]\n",
    "        j=j+1\n",
    "print(dicts)\n",
    "\n",
    "df[3].rename(columns=dicts,\n",
    "          inplace=True)\n",
    "\n",
    "\n",
    "df[3].loc[-1] = warehouse_1  # adding a row\n",
    "df[3].index = df[3].index + 1  # shifting index\n",
    "df[3].sort_index(inplace=True) \n",
    " \n",
    "# call rename () method\n",
    "\n",
    "df[3]\n",
    "df[3].to_csv('../LAKEHOUSE_SILVER_LAYER/warehouse.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5e4902",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ee84b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< Updated upstream
   "display_name": "Python 3.11.0 ('myproject')",
=======
   "display_name": "Python 3 (ipykernel)",
>>>>>>> Stashed changes
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
<<<<<<< Updated upstream
   "version": "3.11.1"
=======
   "version": "3.10.0"
>>>>>>> Stashed changes
  },
  "vscode": {
   "interpreter": {
    "hash": "76d25c3d45d9f36eceb4c77ab90696cff9600176aa43c2b698dbb065297691db"
   }
=======
   "version": "3.11.1"
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
