{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "545a9836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import TimestampType\n",
    "import datetime\n",
    "from pyspark.sql.functions import min, col, date_format\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import first\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import min, col, datediff, current_date\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9722b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Read CSV\").getOrCreate()\n",
    "\n",
    "item = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"LAKEHOUSE_SILVER_LAYER/items1.csv\")\n",
    "prod = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"LAKEHOUSE_SILVER_LAYER/production.csv\")\n",
    "warehouse = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"LAKEHOUSE_SILVER_LAYER/warehouse.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "be347487",
   "metadata": {},
   "outputs": [],
   "source": [
    "modwarehouse = warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d8ac1017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Entry No_',\n",
       " 'Journal Batch Name',\n",
       " 'Line No_',\n",
       " 'Registering Date',\n",
       " 'Location Code',\n",
       " 'Zone Code',\n",
       " 'Bin Code',\n",
       " 'Description',\n",
       " 'Item No_',\n",
       " 'Quantity',\n",
       " 'Qty_ (Base)',\n",
       " 'Source Type',\n",
       " 'Source Subtype',\n",
       " 'Source No_',\n",
       " 'Source Line No_',\n",
       " 'Source Subline No_',\n",
       " 'Source Document',\n",
       " 'Source Code',\n",
       " 'Reason Code',\n",
       " 'No_ Series',\n",
       " 'Bin Type Code',\n",
       " 'Cubage',\n",
       " 'Weight',\n",
       " 'Journal Template Name',\n",
       " 'Whse_ Document No_',\n",
       " 'Whse_ Document Type',\n",
       " 'Whse_ Document Line No_',\n",
       " 'Entry Type',\n",
       " 'Reference Document',\n",
       " 'Reference No_',\n",
       " 'User ID',\n",
       " 'Variant Code',\n",
       " 'Qty_ per Unit of Measure',\n",
       " 'Unit of Measure Code',\n",
       " 'Serial No_',\n",
       " 'Lot No_',\n",
       " 'Warranty Date',\n",
       " 'Expiration Date',\n",
       " 'Phys Invt Counting Period Code',\n",
       " 'Phys Invt Counting Period Type',\n",
       " 'Dedicated',\n",
       " 'datetime']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#string_to_datetime = udf(lambda x: datetime.datetime.strptime(x, \"%d-%m-%Y\"), TimestampType())\n",
    "#modwarehouse = modwarehouse.withColumn(\"datetime\", string_to_datetime(\"Registering Date\"))\n",
    "\n",
    "modwarehouse = modwarehouse.withColumn(\"datetime\", to_timestamp(\"Registering Date\", \"dd-MM-yyyy\"))\n",
    "modwarehouse.columns\n",
    "#modwarehouse.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7008cd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+---------------+--------------------+\n",
      "|       Lot No_|Bin Code|       Item No_|Min_Registering_Date|\n",
      "+--------------+--------+---------------+--------------------+\n",
      "|IDS-21E2-00125|     TMG|       700536-4|          06-03-2021|\n",
      "|IDS-21E2-00418|     IGS|        1000457|          10-02-2022|\n",
      "|IDS-21E2-00372|     IGS|         117019|          11-04-2021|\n",
      "|IDS-21E2-00564|     IGS|         112048|          20-03-2021|\n",
      "|IDS-21E2-00625|     IGS|         102482|          20-01-2022|\n",
      "|IDS-21E2-00560|     TMG|EII-CNS-OT-0025|          04-05-2021|\n",
      "|IDS-21E2-00316|     IGS|         109456|          22-12-2022|\n",
      "|IDS-21E2-00368|     IGS|         131536|          18-01-2022|\n",
      "|IDS-21E2-00565|     TMG|         105581|          23-03-2021|\n",
      "|IDS-21E2-00125|     IGS|         121876|          14-02-2022|\n",
      "|IDS-21E2-00039|     IGS|         116912|          25-03-2021|\n",
      "|IDS-21E2-00337|     TMG|         119852|          13-02-2021|\n",
      "|IDS-21E2-00337|     TMG|         111742|          15-04-2021|\n",
      "|IDS-21E2-00944|     TMG|         133133|          06-11-2021|\n",
      "|IDS-21E2-00692|     TMG|       700536-3|          03-06-2022|\n",
      "|IDS-21E2-00880|     IGS|         131536|          25-03-2022|\n",
      "|IDS-21E2-00496|     IGS|         138531|          29-04-2021|\n",
      "|IDS-21E2-01089|     TMG|         108259|          31-07-2022|\n",
      "|IDS-21E2-00125|     TMG|       700567-1|          31-12-2021|\n",
      "|IDS-21E2-00114|     TMG|         117871|          16-10-2022|\n",
      "+--------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Lot No_: string (nullable = true)\n",
      " |-- Bin Code: string (nullable = true)\n",
      " |-- Item No_: string (nullable = true)\n",
      " |-- Min_Registering_Date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datestep1 = modwarehouse.groupBy(\"Lot No_\", \"Bin Code\", \"Item No_\").agg(min(col(\"datetime\")).alias(\"Min_Registering_Date\"))\n",
    "datestep1 = datestep1.select(\"Lot No_\", \"Bin Code\", \"Item No_\", date_format(col(\"Min_Registering_Date\"), 'dd-MM-yyyy').alias(\"Min_Registering_Date\"))\n",
    "datestep1.show()\n",
    "datestep1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ac382109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------+\n",
      "|Quantity|\n",
      "+--------+\n",
      "|  1900.0|\n",
      "|     0.0|\n",
      "|   375.0|\n",
      "|   450.0|\n",
      "|   250.0|\n",
      "|  1925.0|\n",
      "|   250.0|\n",
      "|     0.0|\n",
      "|     0.0|\n",
      "|   200.0|\n",
      "|   425.0|\n",
      "|   200.0|\n",
      "|   250.0|\n",
      "|     0.0|\n",
      "|   200.0|\n",
      "|   250.0|\n",
      "|   400.0|\n",
      "|   250.0|\n",
      "|     0.0|\n",
      "|     0.0|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warehouse.createOrReplaceTempView(\"warehouse_table\")\n",
    "waretable = spark.table(\"warehouse_table\")\n",
    "print(type(step1))\n",
    "step1temp = waretable.groupBy(\"Lot No_\", \"Bin Code\", \"Item No_\").agg(sum(\"Quantity\").alias(\"Quantity\"))\n",
    "lis = step1temp.select(\"Quantity\")\n",
    "lis.show()\n",
    "type(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bcb396de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+---------+--------+--------------------+\n",
      "|       Lot No_|Bin Code|Item No_|Zone Code|Quantity|Min_Registering_Date|\n",
      "+--------------+--------+--------+---------+--------+--------------------+\n",
      "|IDS-21E2-00028|     IGS| 1000457|      IGS|  1000.0|          19-05-2021|\n",
      "|IDS-21E2-00028|     IGS|  102078|      IGS|   175.0|          08-08-2022|\n",
      "|IDS-21E2-00028|     IGS|  102565|      IGS|   350.0|          28-01-2022|\n",
      "|IDS-21E2-00028|     IGS|  102671|      IGS|   200.0|          13-12-2021|\n",
      "|IDS-21E2-00028|     IGS|  104754|      IGS|     0.0|          24-02-2021|\n",
      "|IDS-21E2-00028|     IGS|  105581|      IGS|  1175.0|          15-05-2021|\n",
      "|IDS-21E2-00028|     IGS|  108259|      IGS|  1625.0|          15-03-2021|\n",
      "|IDS-21E2-00028|     IGS|  111742|      IGS|  1200.0|          16-02-2021|\n",
      "|IDS-21E2-00028|     IGS|  112048|      IGS|   825.0|          27-07-2021|\n",
      "|IDS-21E2-00028|     IGS|  113014|      IGS|   250.0|          27-12-2021|\n",
      "|IDS-21E2-00028|     IGS|  114717|      IGS|   375.0|          17-06-2021|\n",
      "|IDS-21E2-00028|     IGS|  115657|      IGS|     0.0|          12-09-2021|\n",
      "|IDS-21E2-00028|     IGS|  117019|      IGS|     0.0|          21-07-2021|\n",
      "|IDS-21E2-00028|     IGS|  117232|      IGS|   375.0|          10-05-2021|\n",
      "|IDS-21E2-00028|     IGS|  117871|      IGS|     0.0|          14-09-2021|\n",
      "|IDS-21E2-00028|     IGS|  118270|      IGS|   425.0|          17-10-2021|\n",
      "|IDS-21E2-00028|     IGS|  118578|      IGS|   425.0|          06-03-2021|\n",
      "|IDS-21E2-00028|     IGS|  119192|      IGS|   200.0|          17-09-2022|\n",
      "|IDS-21E2-00028|     IGS|  119852|      IGS|  3150.0|          12-02-2021|\n",
      "|IDS-21E2-00028|     IGS|  120958|      IGS|     0.0|          21-12-2022|\n",
      "+--------------+--------+--------+---------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def func(lis):\n",
    "    return sorted(lis.unique())[0]\n",
    "\n",
    "udf_func = udf(func, StringType())\n",
    "step1 = waretable.groupBy(\"Lot No_\", \"Bin Code\", \"Item No_\").agg(min(\"Zone Code\").alias(\"Zone Code\"))\n",
    "step1 = step1.join(step1temp, on=[\"Lot No_\", \"Bin Code\", \"Item No_\"], how='inner')\n",
    "step1 = step1.join(datestep1, on=[\"Lot No_\", \"Bin Code\", \"Item No_\"], how='inner')\n",
    "step1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7c7c929a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "604\n",
      "root\n",
      " |-- Lot No_: string (nullable = true)\n",
      " |-- Bin Code: string (nullable = true)\n",
      " |-- Item No_: string (nullable = true)\n",
      " |-- Zone Code: string (nullable = true)\n",
      " |-- Quantity: double (nullable = true)\n",
      " |-- Min_Registering_Date: string (nullable = true)\n",
      "\n",
      "23/01/13 02:06:53 ERROR Executor: Exception in task 0.0 in stage 252.0 (TID 176)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 670, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 507, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 289, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: code() argument 13 must be str, not int\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/01/13 02:06:53 WARN TaskSetManager: Lost task 0.0 in stage 252.0 (TID 176) (192.168.29.57 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 670, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 507, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 289, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: code() argument 13 must be str, not int\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:86)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:68)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage8.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/01/13 02:06:53 ERROR TaskSetManager: Task 0 in stage 252.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 670, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 507, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 289, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: code() argument 13 must be str, not int\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[133], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#daystep1 = step1.withColumn(\"Min_Registering_Date\",to_date(col(\"Min_Registering_Date\"), 'dd-MM-yyyy'))\u001b[39;00m\n\u001b[1;32m      8\u001b[0m daystep1 \u001b[38;5;241m=\u001b[39m daystep1\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDays from Registration\u001b[39m\u001b[38;5;124m\"\u001b[39m,udffunc(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMin_Registering_Date\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m----> 9\u001b[0m \u001b[43mdaystep1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#daystep1 = daystep1.withColumn(\"Days\", F.to_date(F.col(\"Min_Registering_Date\").cast(\"string\"),\"%Y-%m-%d\").alias(\"date\"))\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#daystep1 = step1.withColumn(\"Days\", col(\"Min_Registering_Date\")))\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#print(current_date())\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#daysstep1.show()\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/sql/dataframe.py:606\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameter \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a bool\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 670, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 507, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 289, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 173, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 471, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: code() argument 13 must be str, not int\n"
     ]
    }
   ],
   "source": [
    "def getdays(date):\n",
    "    return int((datetime.datetime.now() - datetime.datetime.strptime(date, \"%d-%m-%Y\")).days)\n",
    "    \n",
    "print(getdays(\"19-05-2021\"))\n",
    "udffunc = F.udf(getdays)\n",
    "step1.printSchema()\n",
    "#daystep1 = step1.withColumn(\"Min_Registering_Date\",to_date(col(\"Min_Registering_Date\"), 'dd-MM-yyyy'))\n",
    "daystep1 = daystep1.withColumn(\"Days from Registration\",udffunc(col(\"Min_Registering_Date\")))\n",
    "daystep1.show()\n",
    "#daystep1 = daystep1.withColumn(\"Days\", F.to_date(F.col(\"Min_Registering_Date\").cast(\"string\"),\"%Y-%m-%d\").alias(\"date\"))\n",
    "#daystep1 = step1.withColumn(\"Days\", col(\"Min_Registering_Date\")))\n",
    "\n",
    "#print(current_date())\n",
    "#daysstep1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "45f955e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+---------+--------+\n",
      "|       Lot No_|Bin Code|Item No_|Zone Code|Quantity|\n",
      "+--------------+--------+--------+---------+--------+\n",
      "|IDS-21E2-00028|     IGS|  102671|      IGS|   200.0|\n",
      "+--------------+--------+--------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "step1.filter(col(\"Zone Code\") == \"IGS\").filter(col(\"Item No_\") == \"102671\").filter(col(\"Lot No_\") == \"IDS-21E2-00028\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e57eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"new_column\", F.to_date(F.col(\"column\").cast(\"string\"),\"%Y-%m-%d\").alias(\"date\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
