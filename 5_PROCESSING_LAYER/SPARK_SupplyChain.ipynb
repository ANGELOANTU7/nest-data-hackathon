{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "545a9836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import TimestampType\n",
    "import datetime\n",
    "from pyspark.sql.functions import min, col, date_format\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import first\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import to_timestamp, desc\n",
    "from pyspark.sql.functions import min, col, datediff, current_date\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9722b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Read CSV\").getOrCreate()\n",
    "\n",
    "item = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../3_STORAGE_LAYER/DATALAKEHOUSE_BRONZE_LAYER/items1.csv\")\n",
    "prod = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../3_STORAGE_LAYER/DATALAKEHOUSE_BRONZE_LAYER/production.csv\")\n",
    "warehouse = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"../3_STORAGE_LAYER/DATALAKEHOUSE_BRONZE_LAYER/warehouse.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be347487",
   "metadata": {},
   "outputs": [],
   "source": [
    "modwarehouse = warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8ac1017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Entry No_',\n",
       " 'Journal Batch Name',\n",
       " 'Line No_',\n",
       " 'Registering Date',\n",
       " 'Location Code',\n",
       " 'Zone Code',\n",
       " 'Bin Code',\n",
       " 'Description',\n",
       " 'Item No_',\n",
       " 'Quantity',\n",
       " 'Qty_ (Base)',\n",
       " 'Source Type',\n",
       " 'Source Subtype',\n",
       " 'Source No_',\n",
       " 'Source Line No_',\n",
       " 'Source Subline No_',\n",
       " 'Source Document',\n",
       " 'Source Code',\n",
       " 'Reason Code',\n",
       " 'No_ Series',\n",
       " 'Bin Type Code',\n",
       " 'Cubage',\n",
       " 'Weight',\n",
       " 'Journal Template Name',\n",
       " 'Whse_ Document No_',\n",
       " 'Whse_ Document Type',\n",
       " 'Whse_ Document Line No_',\n",
       " 'Entry Type',\n",
       " 'Reference Document',\n",
       " 'Reference No_',\n",
       " 'User ID',\n",
       " 'Variant Code',\n",
       " 'Qty_ per Unit of Measure',\n",
       " 'Unit of Measure Code',\n",
       " 'Serial No_',\n",
       " 'Lot No_',\n",
       " 'Warranty Date',\n",
       " 'Expiration Date',\n",
       " 'Phys Invt Counting Period Code',\n",
       " 'Phys Invt Counting Period Type',\n",
       " 'Dedicated',\n",
       " 'datetime']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#string_to_datetime = udf(lambda x: datetime.datetime.strptime(x, \"%d-%m-%Y\"), TimestampType())\n",
    "#modwarehouse = modwarehouse.withColumn(\"datetime\", string_to_datetime(\"Registering Date\"))\n",
    "\n",
    "modwarehouse = modwarehouse.withColumn(\"datetime\", to_timestamp(\"Registering Date\", \"dd-MM-yyyy\"))\n",
    "modwarehouse.columns\n",
    "#modwarehouse.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7008cd6d",
   "metadata": {},
   "outputs": [
    {
<<<<<<< Updated upstream
=======
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
>>>>>>> Stashed changes
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+---------------+--------------------+\n",
      "|       Lot No_|Bin Code|       Item No_|Min_Registering_Date|\n",
      "+--------------+--------+---------------+--------------------+\n",
      "|IDS-21E2-00125|     TMG|       700536-4|          06-03-2021|\n",
      "|IDS-21E2-00418|     IGS|        1000457|          10-02-2022|\n",
      "|IDS-21E2-00372|     IGS|         117019|          11-04-2021|\n",
      "|IDS-21E2-00564|     IGS|         112048|          20-03-2021|\n",
      "|IDS-21E2-00625|     IGS|         102482|          20-01-2022|\n",
      "|IDS-21E2-00560|     TMG|EII-CNS-OT-0025|          04-05-2021|\n",
      "|IDS-21E2-00316|     IGS|         109456|          22-12-2022|\n",
      "|IDS-21E2-00368|     IGS|         131536|          18-01-2022|\n",
      "|IDS-21E2-00565|     TMG|         105581|          23-03-2021|\n",
      "|IDS-21E2-00125|     IGS|         121876|          14-02-2022|\n",
      "|IDS-21E2-00039|     IGS|         116912|          25-03-2021|\n",
      "|IDS-21E2-00337|     TMG|         119852|          13-02-2021|\n",
      "|IDS-21E2-00337|     TMG|         111742|          15-04-2021|\n",
      "|IDS-21E2-00944|     TMG|         133133|          06-11-2021|\n",
      "|IDS-21E2-00692|     TMG|       700536-3|          03-06-2022|\n",
      "|IDS-21E2-00880|     IGS|         131536|          25-03-2022|\n",
      "|IDS-21E2-00496|     IGS|         138531|          29-04-2021|\n",
      "|IDS-21E2-01089|     TMG|         108259|          31-07-2022|\n",
      "|IDS-21E2-00125|     TMG|       700567-1|          31-12-2021|\n",
      "|IDS-21E2-00114|     TMG|         117871|          16-10-2022|\n",
      "+--------------+--------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Lot No_: string (nullable = true)\n",
      " |-- Bin Code: string (nullable = true)\n",
      " |-- Item No_: string (nullable = true)\n",
      " |-- Min_Registering_Date: string (nullable = true)\n",
      "\n"
     ]
<<<<<<< Updated upstream
=======
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
>>>>>>> Stashed changes
    }
   ],
   "source": [
    "datestep1 = modwarehouse.groupBy(\"Lot No_\", \"Bin Code\", \"Item No_\").agg(min(col(\"datetime\")).alias(\"Min_Registering_Date\"))\n",
    "datestep1 = datestep1.select(\"Lot No_\", \"Bin Code\", \"Item No_\", date_format(col(\"Min_Registering_Date\"), 'dd-MM-yyyy').alias(\"Min_Registering_Date\"))\n",
    "datestep1.show()\n",
    "datestep1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac382109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "+--------+\n",
      "|Quantity|\n",
      "+--------+\n",
      "|  1900.0|\n",
      "|     0.0|\n",
      "|   375.0|\n",
      "|   450.0|\n",
      "|   250.0|\n",
      "|  1925.0|\n",
      "|   250.0|\n",
      "|     0.0|\n",
      "|     0.0|\n",
      "|   200.0|\n",
      "|   425.0|\n",
      "|   200.0|\n",
      "|   250.0|\n",
      "|     0.0|\n",
      "|   200.0|\n",
      "|   250.0|\n",
      "|   400.0|\n",
      "|   250.0|\n",
      "|     0.0|\n",
      "|     0.0|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warehouse.createOrReplaceTempView(\"warehouse_table\")\n",
    "waretable = spark.table(\"warehouse_table\")\n",
    "print(type(waretable))\n",
    "step1temp = waretable.groupBy(\"Lot No_\", \"Bin Code\", \"Item No_\").agg(sum(\"Quantity\").alias(\"Quantity\"))\n",
    "lis = step1temp.select(\"Quantity\")\n",
    "lis.show()\n",
    "type(lis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcb396de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+---------+--------+--------------------+\n",
      "|       Lot No_|Bin Code|Item No_|Zone Code|Quantity|Min_Registering_Date|\n",
      "+--------------+--------+--------+---------+--------+--------------------+\n",
      "|IDS-21E2-00028|     IGS| 1000457|      IGS|  1000.0|          19-05-2021|\n",
      "|IDS-21E2-00028|     IGS|  102078|      IGS|   175.0|          08-08-2022|\n",
      "|IDS-21E2-00028|     IGS|  102565|      IGS|   350.0|          28-01-2022|\n",
      "|IDS-21E2-00028|     IGS|  102671|      IGS|   200.0|          13-12-2021|\n",
      "|IDS-21E2-00028|     IGS|  104754|      IGS|     0.0|          24-02-2021|\n",
      "|IDS-21E2-00028|     IGS|  105581|      IGS|  1175.0|          15-05-2021|\n",
      "|IDS-21E2-00028|     IGS|  108259|      IGS|  1625.0|          15-03-2021|\n",
      "|IDS-21E2-00028|     IGS|  111742|      IGS|  1200.0|          16-02-2021|\n",
      "|IDS-21E2-00028|     IGS|  112048|      IGS|   825.0|          27-07-2021|\n",
      "|IDS-21E2-00028|     IGS|  113014|      IGS|   250.0|          27-12-2021|\n",
      "|IDS-21E2-00028|     IGS|  114717|      IGS|   375.0|          17-06-2021|\n",
      "|IDS-21E2-00028|     IGS|  115657|      IGS|     0.0|          12-09-2021|\n",
      "|IDS-21E2-00028|     IGS|  117019|      IGS|     0.0|          21-07-2021|\n",
      "|IDS-21E2-00028|     IGS|  117232|      IGS|   375.0|          10-05-2021|\n",
      "|IDS-21E2-00028|     IGS|  117871|      IGS|     0.0|          14-09-2021|\n",
      "|IDS-21E2-00028|     IGS|  118270|      IGS|   425.0|          17-10-2021|\n",
      "|IDS-21E2-00028|     IGS|  118578|      IGS|   425.0|          06-03-2021|\n",
      "|IDS-21E2-00028|     IGS|  119192|      IGS|   200.0|          17-09-2022|\n",
      "|IDS-21E2-00028|     IGS|  119852|      IGS|  3150.0|          12-02-2021|\n",
      "|IDS-21E2-00028|     IGS|  120958|      IGS|     0.0|          21-12-2022|\n",
      "+--------------+--------+--------+---------+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def func(lis):\n",
    "    return sorted(lis.unique())[0]\n",
    "\n",
    "udf_func = udf(func, StringType())\n",
    "step1 = waretable.groupBy(\"Lot No_\", \"Bin Code\", \"Item No_\").agg(min(\"Zone Code\").alias(\"Zone Code\"))\n",
    "step1 = step1.join(step1temp, on=[\"Lot No_\", \"Bin Code\", \"Item No_\"], how='inner')\n",
    "step1 = step1.join(datestep1, on=[\"Lot No_\", \"Bin Code\", \"Item No_\"], how='inner')\n",
    "step1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6637e90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+---------+--------+--------------------+----------------------+\n",
      "|       Lot No_|Bin Code|Item No_|Zone Code|Quantity|Min_Registering_Date|Days from Registration|\n",
      "+--------------+--------+--------+---------+--------+--------------------+----------------------+\n",
      "|IDS-21E2-00028|     IGS| 1000457|      IGS|  1000.0|          2021-05-19|                   606|\n",
      "|IDS-21E2-00028|     IGS|  102078|      IGS|   175.0|          2022-08-08|                   160|\n",
      "|IDS-21E2-00028|     IGS|  102565|      IGS|   350.0|          2022-01-28|                   352|\n",
      "|IDS-21E2-00028|     IGS|  102671|      IGS|   200.0|          2021-12-13|                   398|\n",
      "|IDS-21E2-00028|     IGS|  104754|      IGS|     0.0|          2021-02-24|                   690|\n",
      "|IDS-21E2-00028|     IGS|  105581|      IGS|  1175.0|          2021-05-15|                   610|\n",
      "|IDS-21E2-00028|     IGS|  108259|      IGS|  1625.0|          2021-03-15|                   671|\n",
      "|IDS-21E2-00028|     IGS|  111742|      IGS|  1200.0|          2021-02-16|                   698|\n",
      "|IDS-21E2-00028|     IGS|  112048|      IGS|   825.0|          2021-07-27|                   537|\n",
      "|IDS-21E2-00028|     IGS|  113014|      IGS|   250.0|          2021-12-27|                   384|\n",
      "|IDS-21E2-00028|     IGS|  114717|      IGS|   375.0|          2021-06-17|                   577|\n",
      "|IDS-21E2-00028|     IGS|  115657|      IGS|     0.0|          2021-09-12|                   490|\n",
      "|IDS-21E2-00028|     IGS|  117019|      IGS|     0.0|          2021-07-21|                   543|\n",
      "|IDS-21E2-00028|     IGS|  117232|      IGS|   375.0|          2021-05-10|                   615|\n",
      "|IDS-21E2-00028|     IGS|  117871|      IGS|     0.0|          2021-09-14|                   488|\n",
      "|IDS-21E2-00028|     IGS|  118270|      IGS|   425.0|          2021-10-17|                   455|\n",
      "|IDS-21E2-00028|     IGS|  118578|      IGS|   425.0|          2021-03-06|                   680|\n",
      "|IDS-21E2-00028|     IGS|  119192|      IGS|   200.0|          2022-09-17|                   120|\n",
      "|IDS-21E2-00028|     IGS|  119852|      IGS|  3150.0|          2021-02-12|                   702|\n",
      "|IDS-21E2-00028|     IGS|  120958|      IGS|     0.0|          2022-12-21|                    25|\n",
      "+--------------+--------+--------+---------+--------+--------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "step1 = step1.withColumn(\"Min_Registering_Date\", to_date(col(\"Min_Registering_Date\"), 'dd-MM-yyyy'))\n",
    "step1 = step1.withColumn(\"Days from Registration\", datediff(current_date(),col(\"Min_Registering_Date\")))\n",
    "\n",
    "step1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bb92d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step1 = step1.filter(step1[\"Quantity\"] > 0)\n",
    "step1.count()\n",
    "len(step1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c929a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45f955e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+---------+--------+--------------------+----------------------+\n",
      "|       Lot No_|Bin Code|Item No_|Zone Code|Quantity|Min_Registering_Date|Days from Registration|\n",
      "+--------------+--------+--------+---------+--------+--------------------+----------------------+\n",
      "|IDS-21E2-00028|     IGS|  102671|      IGS|   200.0|          2021-12-13|                   398|\n",
      "+--------------+--------+--------+---------+--------+--------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "step1.filter(col(\"Zone Code\") == \"IGS\").filter(col(\"Item No_\") == \"102671\").filter(col(\"Lot No_\") == \"IDS-21E2-00028\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05e57eb4",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/c:/Users/Nasim/Desktop/mishal/nest-data-hackathon/6_SERVING_LAYER/LAKEHOUSE_GOLD_LAYER/sclatest_step1.csv already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m step1\u001b[39m.\u001b[39;49mcoalesce(\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mformat(\u001b[39m'\u001b[39;49m\u001b[39mcom.databricks.spark.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49moptions(header\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrue\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39;49m\u001b[39m../6_SERVING_LAYER/LAKEHOUSE_GOLD_LAYER/sclatest_step1.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Nasim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:968\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave()\n\u001b[0;32m    967\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave(path)\n",
      "File \u001b[1;32mc:\\Users\\Nasim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\Nasim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: path file:/c:/Users/Nasim/Desktop/mishal/nest-data-hackathon/6_SERVING_LAYER/LAKEHOUSE_GOLD_LAYER/sclatest_step1.csv already exists."
     ]
    }
   ],
   "source": [
    "step1.coalesce(1).write.format('com.databricks.spark.csv').options(header='true').save('../6_SERVING_LAYER/LAKEHOUSE_GOLD_LAYER/sclatest_step1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ade547f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c486ec90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows before removing NA values:  12000\n",
      "Number of rows after removing NA values:  8049\n",
      "+--------+-----+--------------------+--------------------+-------------+--------------------+---------------------+----+-----------------------+---------+----------------+-------------------+----------------+----------------+----------+------------------------+--------+--------------+---------+-------------+-----------------+-----------------+---------------+--------------+------------------+\n",
      "|     No_|No_ 2|         Description|  Search Description|Description 2|Base Unit of Measure|Price Unit Conversion|Type|Inventory Posting Group|Shelf No_|Item Disc_ Group|Allow Invoice Disc_|Statistics Group|Commission Group|Unit Price|Price_Profit Calculation|Profit _|Costing Method|Unit Cost|Standard Cost|Quoted Price(INR)|Quoted Price(FCY)|Quoted Currency|Standard Cost_|Production BOM No_|\n",
      "+--------+-----+--------------------+--------------------+-------------+--------------------+---------------------+----+-----------------------+---------+----------------+-------------------+----------------+----------------+----------+------------------------+--------+--------------+---------+-------------+-----------------+-----------------+---------------+--------------+------------------+\n",
      "|  135677| null|STATIC SHIELDED B...|STATIC SHIELDED B...|         null|                  EA|                  120|   0|                     CN|     S001|            D667|                  1|              G1|              C1|       486|                     536|      50|            UC|     2541|         2554|               31|               26|            USD|            86|       1000456-SMT|\n",
      "|  119852| null|STATIC SHIELDED B...|STATIC SHIELDED B...|         null|                  EA|                  120|   0|                     RM|     S001|            D654|                  1|              G1|              C1|       251|                     301|      50|            UC|     3799|         3799|               16|               49|            USD|            86|       1000456-SMT|\n",
      "|  147831| null|RoHS,Label, 4X6 inch|RoHS,Label, 4X6 inch|         null|                  EA|                  120|   0|                     PM|     S003|            D667|                  1|              G1|              C1|       264|                     314|      50|            UC|     3772|         3872|               50|                6|            USD|            82|       1000456-SMT|\n",
      "|  105695| null|RoHSLabel, 3.5 X ...|RoHSLabel, 3.5 X ...|         null|                  EA|                  120|   0|                     PM|     S007|            D666|                  1|              G1|              C1|       418|                     468|      50|            UC|     2807|         2861|               29|                9|            INR|            88|       1000456-SMT|\n",
      "|  123546| null|         RoHS,DRAM-1|RoHS,LABEL-CRUCIA...|         null|                  EA|                  120|   0|                     PM|     S003|            D654|                  1|              G1|              C1|       287|                     337|      50|            UC|     2823|         2824|               50|                5|            INR|            91|       1000457-SMT|\n",
      "|  138531| null|STATIC SHIELDED B...|STATIC SHIELDED B...|         null|                  EA|                  120|   0|                     PM|     S001|            D655|                  1|              G1|              C1|       594|                     644|      50|            UC|     3919|         3509|               16|               14|            USD|            83|       1000456-SMT|\n",
      "|700536-4| null|OTCT-650*478*228M...|OTCT-650*478*228M...|         null|                  EA|                  120|   0|                     RM|     S003|            D666|                  1|              G1|              C1|       647|                     697|      50|            UC|     2382|         2706|               37|                5|            INR|            98|       1000456-SMT|\n",
      "|  106328| null|         RoHS,DRAM-1|         RoHS,DRAM-2|         null|                  EA|                  120|   0|                     CN|     S001|            D666|                  1|              G1|              C1|       616|                     666|      50|            UC|     2825|         2825|                4|                7|            USD|            88|       1000457-SMT|\n",
      "|  100441| null|RoHS PCB BOARD, I...|RoHS PCB BOARD, I...|         null|                  EA|                  120|   0|                     RM|     S010|            D654|                  1|              G1|              C1|       283|                     333|      50|            UC|     2815|         2815|               24|               28|            USD|            96|       1000456-SMT|\n",
      "|  105695| null|STATIC SHIELDED B...|STATIC SHIELDED B...|         null|                  EA|                  120|   0|                     RM|     S007|            D655|                  1|              G1|              C1|       424|                     474|      50|            UC|     3798|         3098|               31|               47|            INR|            76|       1000456-SMT|\n",
      "|  114717| null|OTCT-377*192*309M...|OTCT-377*192*309M...|         null|                  EA|                  120|   0|                     PM|     S002|            D666|                  1|              G1|              C1|       540|                     590|      50|            UC|     2748|         2748|               20|               26|            USD|            78|       1000457-SMT|\n",
      "|  135506| null|      FUJI-FILTER 02|RoHSLabel, 3.5 X ...|         null|                  EA|                  120|   0|                     PM|     S004|            D668|                  1|              G1|              C1|       402|                     452|      50|            UC|     2551|         2973|               36|                2|            INR|            80|       1000457-SMT|\n",
      "|  145433| null|RoHS,Label, 5 X 2...|RoHS,Label, 5 X 2...|         null|                  EA|                  120|   0|                     CN|     S008|            D655|                  1|              G1|              C1|       640|                     690|      50|            UC|     3931|         3536|               25|               29|            INR|            95|       1000456-SMT|\n",
      "|  146940| null|OTCT-486*377*309M...|OTCT-486*377*309M...|         null|                  EA|                  120|   0|                     RM|     S002|            D668|                  1|              G1|              C1|       655|                     705|      50|            UC|     2535|         2525|               22|               50|            USD|            82|       1000456-SMT|\n",
      "|  122829| null|OTCT-377*192*309M...|OTCT-377*192*309M...|         null|                  EA|                  120|   0|                     PM|     S010|            D654|                  1|              G1|              C1|       503|                     553|      50|            UC|     3233|         3237|               18|               26|            INR|            99|       1000457-SMT|\n",
      "|  121571| null|         RoHS,DRAM-1|         RoHS,DRAM-2|         null|                  EA|                  120|   0|                     PM|     S002|            D668|                  1|              G1|              C1|       541|                     591|      50|            UC|     3141|         3471|               39|               11|            INR|            75|       1000456-SMT|\n",
      "|  108259| null|RoHS,Label, 4X6 inch|RoHS,Label, 4X6 inch|         null|                  EA|                  120|   0|                     PM|     S002|            D668|                  1|              G1|              C1|       700|                     750|      50|            UC|     3858|         3845|               58|               17|            USD|            93|       1000457-SMT|\n",
      "|  108259| null|RoHSLabel, 3.5 X ...|RoHSLabel, 3.5 X ...|         null|                  EA|                  120|   0|                     RM|     S005|            D655|                  1|              G1|              C1|       326|                     376|      50|            UC|     3190|         3190|               41|                4|            INR|            81|       1000457-SMT|\n",
      "|  127229| null|RoHS,LABEL-CRUCIA...|RoHS,LABEL-CRUCIA...|         null|                  EA|                  120|   0|                     CN|     S006|            D655|                  1|              G1|              C1|       743|                     793|      50|            UC|     3357|         3254|                1|               10|            INR|            94|       1000456-SMT|\n",
      "|  108259| null|  RoHS,Security Tape|  RoHS,Security Tape|         null|                  EA|                  120|   0|                     PM|     S004|            D654|                  1|              G1|              C1|       252|                     302|      50|            UC|     3661|         3663|               53|               17|            USD|            82|       1000456-SMT|\n",
      "+--------+-----+--------------------+--------------------+-------------+--------------------+---------------------+----+-----------------------+---------+----------------+-------------------+----------------+----------------+----------+------------------------+--------+--------------+---------+-------------+-----------------+-----------------+---------------+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of rows before removing NA values: \", item.count())\n",
    "item = item.dropna(subset=['Production BOM No_'])\n",
    "print(\"Number of rows after removing NA values: \", item.count())\n",
    "item.show()\n",
    "len(item.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e9f6bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+--------+----+--------------------+--------+--------------------+--------+--------+----------+----------+----------------+-----------------+-------+------------+-------------+-----------+------+-----+------+-----+-------------------+------------+---------------+-------+--------+\n",
      "|Production BOM No_|Version Code|Line No_|Type|         Description|     No_|Unit of Measure Code|Quantity|Position|Position 2|Position 3|Lead-Time Offset|Routing Link Code|Scrap _|Variant Code|Starting Date|Ending Date|Length|Width|Weight|Depth|Calculation Formula|Quantity per|Principal Input|Company|Division|\n",
      "+------------------+------------+--------+----+--------------------+--------+--------------------+--------+--------+----------+----------+----------------+-----------------+-------+------------+-------------+-----------+------+-----+------+-----+-------------------+------------+---------------+-------+--------+\n",
      "|       1000456-SMT|        null|11000000|   1|STATIC SHIELDED B...|  135677|                  EA|      16|     SMT|      null|      null|              12|              AOI|      0|           L|   26-01-2021| 28-07-2026|    13|   14|    17|   19|                  0|          65|              1|   null|    null|\n",
      "|       1000456-SMT|        null|10277800|   1|STATIC SHIELDED B...|  119852|                  EA|      66|     SMT|      null|      null|              15|              AOI|      0|           M|   23-04-2020| 14-10-2026|    16|   16|    11|   18|                  0|          99|              1|   null|    null|\n",
      "|       1000456-SMT|        null| 5837686|   1|RoHS,Label, 4X6 inch|  147831|                  EA|      99|     SMT|      null|      null|              13|              AOI|      0|           M|   29-01-2020| 19-06-2026|    19|   13|    10|   20|                  0|          59|              1|   null|    null|\n",
      "|       1000456-SMT|        null| 9996220|   1|RoHSLabel, 3.5 X ...|  105695|                  EA|       1|     SMT|      null|      null|              13|              AOI|      0|           S|   30-06-2020| 27-04-2026|    15|   11|    12|   20|                  0|          66|              1|   null|    null|\n",
      "|              null|        null|11000000|   1|  RoHS,Security Tape|  141663|                  EA|      38|     SMT|      null|      null|              12|              AOI|      0|           M|   26-08-2020| 27-03-2026|    11|   10|    20|   19|                  0|          72|              1|   null|    null|\n",
      "|       1000457-SMT|        null| 8566309|   1|         RoHS,DRAM-1|  123546|                  EA|      98|     SMT|      null|      null|              15|              AOI|      0|           M|   14-10-2020| 18-11-2026|    10|   13|    13|   17|                  0|          52|              1|   null|    null|\n",
      "|       1000456-SMT|        null| 4329487|   1|STATIC SHIELDED B...|  138531|                  EA|      96|     SMT|      null|      null|              10|              AOI|      0|           L|   14-10-2020| 30-12-2026|    11|   13|    12|   20|                  0|          87|              1|   null|    null|\n",
      "|       1000456-SMT|        null| 4806327|   1|OTCT-650*478*228M...|700536-4|                  EA|      64|     SMT|      null|      null|              10|              AOI|      0|           L|   12-02-2020| 15-06-2026|    19|   14|    11|   10|                  0|           5|              1|   null|    null|\n",
      "|              null|        null| 8686827|   1|RoHS CARTON,SINGL...|  108259|                  EA|      99|     SMT|      null|      null|              13|              AOI|      0|           M|   06-05-2020| 19-06-2026|    18|   20|    19|   15|                  0|          18|              1|   null|    null|\n",
      "|       1000457-SMT|        null|11000000|   1|         RoHS,DRAM-1|  106328|                  EA|      99|     SMT|      null|      null|              14|              AOI|      0|           S|   16-10-2020| 28-07-2026|    19|   14|    12|   16|                  0|          61|              1|   null|    null|\n",
      "|              null|        null|  722474|   1|              _RARE_| 1000457|                  EA|      99|     SMT|      null|      null|              13|              AOI|      0|           M|   17-11-2020| 06-01-2027|    18|   17|    16|   20|                  0|          50|              1|   null|    null|\n",
      "|       1000456-SMT|        null| 6238162|   1|RoHS PCB BOARD, I...|  100441|                  EA|      85|     SMT|      null|      null|              13|              AOI|      0|           L|   16-03-2020| 04-03-2026|    16|   20|    11|   15|                  0|          12|              1|   null|    null|\n",
      "|       1000456-SMT|        null| 5469092|   1|STATIC SHIELDED B...|  105695|                  EA|      99|     SMT|      null|      null|              12|              AOI|      0|           L|   31-03-2020| 08-12-2026|    11|   12|    17|   20|                  0|          50|              1|   null|    null|\n",
      "|       1000457-SMT|        null|   10000|   1|OTCT-377*192*309M...|  114717|                  EA|      99|     SMT|      null|      null|              10|              AOI|      0|           S|   12-02-2020| 08-09-2026|    18|   10|    16|   17|                  0|          18|              1|   null|    null|\n",
      "|       1000457-SMT|        null|   10000|   1|      FUJI-FILTER 02|  135506|                  EA|      14|     SMT|      null|      null|              12|              AOI|      0|           S|   23-01-2020| 26-01-2026|    18|   12|    14|   20|                  0|          91|              1|   null|    null|\n",
      "|              null|        null|11000000|   1|OTCT-377*192*309M...|  108259|                  EA|      99|     SMT|      null|      null|              14|              AOI|      0|           M|   11-03-2020| 02-06-2026|    11|   16|    20|   11|                  0|          73|              1|   null|    null|\n",
      "|       1000456-SMT|        null| 1133009|   1|RoHS,Label, 5 X 2...|  145433|                  EA|       5|     SMT|      null|      null|              15|              AOI|      0|           L|   27-04-2020| 09-10-2026|    11|   16|    20|   17|                  0|          99|              1|   null|    null|\n",
      "|              null|        null|   10000|   1|RoHSLabel, 3.5 X ...| 1000457|                  EA|      67|     SMT|      null|      null|              15|              AOI|      0|           M|   23-10-2020| 08-04-2026|    13|   20|    12|   18|                  0|          41|              1|   null|    null|\n",
      "|       1000456-SMT|        null| 7451984|   1|OTCT-486*377*309M...|  146940|                   L|      99|     SMT|      null|      null|              14|              AOI|      0|           M|   03-02-2020| 11-08-2026|    11|   20|    18|   17|                  0|          48|              1|   null|    null|\n",
      "|       1000457-SMT|        null| 2208164|   1|OTCT-377*192*309M...|  122829|                  EA|      87|     SMT|      null|      null|              10|              AOI|      0|           L|   11-05-2020| 28-08-2026|    11|   15|    15|   11|                  0|           1|              1|   null|    null|\n",
      "+------------------+------------+--------+----+--------------------+--------+--------------------+--------+--------+----------+----------+----------------+-----------------+-------+------------+-------------+-----------+------+-----+------+-----+-------------------+------------+---------------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prod.show()\n",
    "prod.count()\n",
    "len(prod.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54013264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0e6e541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|       Item No_|\n",
      "+---------------+\n",
      "|         149104|\n",
      "|         118502|\n",
      "|         124527|\n",
      "|         700527|\n",
      "|         123248|\n",
      "|         106328|\n",
      "|         129315|\n",
      "|         111742|\n",
      "|         148935|\n",
      "|         111854|\n",
      "|         116054|\n",
      "|         145433|\n",
      "|         131156|\n",
      "|         104936|\n",
      "|         112806|\n",
      "|EII-CNS-CC-0002|\n",
      "|         132646|\n",
      "|         115870|\n",
      "|         131319|\n",
      "|         119027|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "333"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step2 = item.alias(\"item\").join(prod.alias(\"prod\"), col(\"item.No_\") == col(\"prod.No_\"), \"inner\")\n",
    "#step2.printSchema()\n",
    "step2 = step2.select(\"item.No_\").dropDuplicates()\n",
    "step2 = step2.withColumnRenamed(\"No_\", \"Item No_\")\n",
    "step2.show()\n",
    "step2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a0a233e",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o189.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$25(FileFormatWriter.scala:267)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:267)\r\n\t... 39 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m step2\u001b[39m.\u001b[39;49mcoalesce(\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mformat(\u001b[39m'\u001b[39;49m\u001b[39mcom.databricks.spark.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49moptions(header\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrue\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39;49m\u001b[39m../6_SERVING_LAYER/LAKEHOUSE_GOLD_LAYER/scNO_step2.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Nasim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:968\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave()\n\u001b[0;32m    967\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave(path)\n",
      "File \u001b[1;32mc:\\Users\\Nasim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\Nasim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Nasim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o189.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$25(FileFormatWriter.scala:267)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:267)\r\n\t... 39 more\r\n"
     ]
    }
   ],
   "source": [
    "step2.coalesce(1).write.format('com.databricks.spark.csv').options(header='true').save('../6_SERVING_LAYER/LAKEHOUSE_GOLD_LAYER/scNO_step2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d5241d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cdddc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------+---------+--------+--------------------+----------------------+\n",
      "|Item No_|       Lot No_|Bin Code|Zone Code|Quantity|Min_Registering_Date|Days from Registration|\n",
      "+--------+--------------+--------+---------+--------+--------------------+----------------------+\n",
      "|  111742|IDS-21E2-00028|     IGS|      IGS|  1200.0|          2021-02-16|                   698|\n",
      "|  131156|IDS-21E2-00028|     IGS|      IGS|   175.0|          2021-10-06|                   466|\n",
      "|  105581|IDS-21E2-00028|     IGS|      IGS|  1175.0|          2021-05-15|                   610|\n",
      "|  119192|IDS-21E2-00028|     IGS|      IGS|   200.0|          2022-09-17|                   120|\n",
      "|  113014|IDS-21E2-00028|     IGS|      IGS|   250.0|          2021-12-27|                   384|\n",
      "|  125168|IDS-21E2-00028|     IGS|      IGS|   200.0|          2021-12-24|                   387|\n",
      "|  118270|IDS-21E2-00028|     IGS|      IGS|   425.0|          2021-10-17|                   455|\n",
      "|  102671|IDS-21E2-00028|     IGS|      IGS|   200.0|          2021-12-13|                   398|\n",
      "|  114717|IDS-21E2-00028|     IGS|      IGS|   375.0|          2021-06-17|                   577|\n",
      "|  112048|IDS-21E2-00028|     IGS|      IGS|   825.0|          2021-07-27|                   537|\n",
      "|  102078|IDS-21E2-00028|     IGS|      IGS|   175.0|          2022-08-08|                   160|\n",
      "|  108259|IDS-21E2-00028|     IGS|      IGS|  1625.0|          2021-03-15|                   671|\n",
      "| 1000457|IDS-21E2-00028|     IGS|      IGS|  1000.0|          2021-05-19|                   606|\n",
      "|  127286|IDS-21E2-00028|     IGS|      IGS|   200.0|          2022-10-20|                    87|\n",
      "|  102565|IDS-21E2-00028|     IGS|      IGS|   350.0|          2022-01-28|                   352|\n",
      "|  117232|IDS-21E2-00028|     IGS|      IGS|   375.0|          2021-05-10|                   615|\n",
      "|  121969|IDS-21E2-00028|     IGS|      IGS|   375.0|          2021-04-25|                   630|\n",
      "|  123625|IDS-21E2-00028|     IGS|      IGS|   175.0|          2022-09-20|                   117|\n",
      "|  118578|IDS-21E2-00028|     IGS|      IGS|   425.0|          2021-03-06|                   680|\n",
      "|  123759|IDS-21E2-00028|     IGS|      IGS|   375.0|          2021-05-10|                   615|\n",
      "+--------+--------------+--------+---------+--------+--------------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "(3994, 7)\n"
     ]
    }
   ],
   "source": [
    "# Perform left join on 'Item No_' column\n",
    "step3 = step1.join(step2, on='Item No_', how='left')\n",
    "\n",
    "# Drop columns that end with '_DROP'\n",
    "drop_columns = [col for col in step3.columns if col.endswith('_DROP')]\n",
    "step3 = step3.drop(*drop_columns)\n",
    "\n",
    "step3.show()\n",
    "# Print shape of step3\n",
    "print((step3.count(), len(step3.columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c648dfff",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o203.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$25(FileFormatWriter.scala:267)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:267)\r\n\t... 39 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m step3\u001b[39m.\u001b[39;49mcoalesce(\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mformat(\u001b[39m'\u001b[39;49m\u001b[39mcom.databricks.spark.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49moptions(header\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrue\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39;49m\u001b[39m../6_SERVING_LAYER/LAKEHOUSE_GOLD_LAYER/sclatest_step3.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Nasim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:968\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave()\n\u001b[0;32m    967\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave(path)\n",
      "File \u001b[1;32mc:\\Users\\Nasim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\Nasim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Nasim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o203.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$25(FileFormatWriter.scala:267)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:267)\r\n\t... 39 more\r\n"
     ]
    }
   ],
   "source": [
    "step3.coalesce(1).write.format('com.databricks.spark.csv').options(header='true').save('../6_SERVING_LAYER/LAKEHOUSE_GOLD_LAYER/sclatest_step3.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e2f8de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Number of rows in step4:  655789\n",
      "3994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8049"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step4 = item.join(step3,item[\"No_\"] == step3[\"Item No_\"], \"left_outer\")\n",
    "\n",
    "\n",
    "\n",
    "#step4 = item.where(item.No_.isin(step3.select(\"Item No_\").collect()))\n",
    "equals = step4.select(F.col(\"Item No_\").eqNullSafe(F.col(\"No_\"))).first()[0]\n",
    "print(equals)\n",
    "if(equals == True):\n",
    "    step4 = step4.drop(\"No_\")\n",
    "print(\"Number of rows in step4: \", step4.count())\n",
    "\n",
    "len(step4.columns)\n",
    "print(step3.count())\n",
    "item.count()\n",
    "#item.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c754c62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o223.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$25(FileFormatWriter.scala:267)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:267)\r\n\t... 39 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m step4\u001b[39m.\u001b[39;49mcoalesce(\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mformat(\u001b[39m'\u001b[39;49m\u001b[39mcom.databricks.spark.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49moptions(header\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrue\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39;49m\u001b[39m../6_SERVING_LAYER/LAKEHOUSE_GOLD_LAYER/scfinal_step4.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Nasim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:968\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave()\n\u001b[0;32m    967\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave(path)\n",
      "File \u001b[1;32mc:\\Users\\Nasim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\Nasim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\Nasim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o223.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:651)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:278)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1589)\r\nCaused by: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$25(FileFormatWriter.scala:267)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:267)\r\n\t... 39 more\r\n"
     ]
    }
   ],
   "source": [
    "step4.coalesce(1).write.format('com.databricks.spark.csv').options(header='true').save('../6_SERVING_LAYER/LAKEHOUSE_GOLD_LAYER/scfinal_step4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0f7e8cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------+---------+--------+--------------------+----------------------+-----+--------------------+--------------------+-------------+--------------------+---------------------+----+-----------------------+---------+----------------+-------------------+----------------+----------------+----------+------------------------+--------+--------------+---------+-------------+-----------------+-----------------+---------------+--------------+------------------+\n",
      "|Item No_|       Lot No_|Bin Code|Zone Code|Quantity|Min_Registering_Date|Days from Registration|No_ 2|         Description|  Search Description|Description 2|Base Unit of Measure|Price Unit Conversion|Type|Inventory Posting Group|Shelf No_|Item Disc_ Group|Allow Invoice Disc_|Statistics Group|Commission Group|Unit Price|Price_Profit Calculation|Profit _|Costing Method|Unit Cost|Standard Cost|Quoted Price(INR)|Quoted Price(FCY)|Quoted Currency|Standard Cost_|Production BOM No_|\n",
      "+--------+--------------+--------+---------+--------+--------------------+----------------------+-----+--------------------+--------------------+-------------+--------------------+---------------------+----+-----------------------+---------+----------------+-------------------+----------------+----------------+----------+------------------------+--------+--------------+---------+-------------+-----------------+-----------------+---------------+--------------+------------------+\n",
      "|  149104|IDS-21E2-00039|     TMG|      TMG|   375.0|          2022-08-19|                   149| null|  RoHS,Security Tape|  RoHS,Security Tape|         null|                  EA|                  120|   0|                     CN|     S004|            D654|                  1|              G1|              C1|       341|                     391|      50|            UC|     3827|         3029|               26|               50|            INR|            81|       1000457-SMT|\n",
      "|  149104|IDS-21E2-00039|     TMG|      TMG|   375.0|          2022-08-19|                   149| null|INCT-CB-345*293*9...|INCT-CB-345*293*9...|         null|                  EA|                  120|   0|                     PM|     S006|            D668|                  1|              G1|              C1|       389|                     439|      50|            UC|     2833|         2861|               42|               48|            INR|            97|       1000456-SMT|\n",
      "|  149104|IDS-21E2-00039|     TMG|      TMG|   375.0|          2022-08-19|                   149| null|RoHS,SCM1G8Z41CD8...|RoHS,SCM1G8Z41CD8...|         null|                  EA|                  120|   0|                     RM|     S010|            D667|                  1|              G1|              C1|       343|                     393|      50|            UC|     3281|         3281|                4|               41|            USD|            87|       1000457-SMT|\n",
      "|  149104|IDS-21E2-00039|     TMG|      TMG|   375.0|          2022-08-19|                   149| null|RoHS CARTON,SINGL...|RoHS CARTON,SINGL...|         null|                  EA|                  120|   0|                     RM|     S007|            D668|                  1|              G1|              C1|       449|                     499|      50|            UC|     3477|         3497|               30|               14|            INR|            99|       1000457-SMT|\n",
      "|  149104|IDS-21E2-00039|     TMG|      TMG|   375.0|          2022-08-19|                   149| null|         RoHS,DRAM-1|         RoHS,DRAM-2|         null|                  EA|                  120|   0|                     CN|     S006|            D655|                  1|              G1|              C1|       553|                     603|      50|            UC|     2822|         2828|               40|               55|            USD|            92|       1000457-SMT|\n",
      "|  149104|IDS-21E2-00056|     IGS|      IGS|   175.0|          2022-12-05|                    41| null|  RoHS,Security Tape|  RoHS,Security Tape|         null|                  EA|                  120|   0|                     CN|     S004|            D654|                  1|              G1|              C1|       341|                     391|      50|            UC|     3827|         3029|               26|               50|            INR|            81|       1000457-SMT|\n",
      "|  149104|IDS-21E2-00056|     IGS|      IGS|   175.0|          2022-12-05|                    41| null|INCT-CB-345*293*9...|INCT-CB-345*293*9...|         null|                  EA|                  120|   0|                     PM|     S006|            D668|                  1|              G1|              C1|       389|                     439|      50|            UC|     2833|         2861|               42|               48|            INR|            97|       1000456-SMT|\n",
      "|  149104|IDS-21E2-00056|     IGS|      IGS|   175.0|          2022-12-05|                    41| null|RoHS,SCM1G8Z41CD8...|RoHS,SCM1G8Z41CD8...|         null|                  EA|                  120|   0|                     RM|     S010|            D667|                  1|              G1|              C1|       343|                     393|      50|            UC|     3281|         3281|                4|               41|            USD|            87|       1000457-SMT|\n",
      "|  149104|IDS-21E2-00056|     IGS|      IGS|   175.0|          2022-12-05|                    41| null|RoHS CARTON,SINGL...|RoHS CARTON,SINGL...|         null|                  EA|                  120|   0|                     RM|     S007|            D668|                  1|              G1|              C1|       449|                     499|      50|            UC|     3477|         3497|               30|               14|            INR|            99|       1000457-SMT|\n",
      "|  149104|IDS-21E2-00056|     IGS|      IGS|   175.0|          2022-12-05|                    41| null|         RoHS,DRAM-1|         RoHS,DRAM-2|         null|                  EA|                  120|   0|                     CN|     S006|            D655|                  1|              G1|              C1|       553|                     603|      50|            UC|     2822|         2828|               40|               55|            USD|            92|       1000457-SMT|\n",
      "|  118502|IDS-21E2-00080|     IGS|      IGS|   250.0|          2022-11-11|                    65| null|OTCT-486*377*309M...|OTCT-486*377*309M...|         null|                  EA|                  120|   0|                     RM|     S004|            D655|                  1|              G1|              C1|       691|                     741|      50|            UC|     3220|         3230|               30|               24|            INR|            90|       1000456-SMT|\n",
      "|  118502|IDS-21E2-00080|     IGS|      IGS|   250.0|          2022-11-11|                    65| null|RoHS CARTON,SINGL...|RoHS CARTON,SINGL...|         null|                  EA|                  120|   0|                     RM|     S002|            D668|                  1|              G1|              C1|       678|                     728|      50|            UC|     3428|         3428|               26|               20|            USD|            95|       1000456-SMT|\n",
      "|  118502|IDS-21E2-00080|     IGS|      IGS|   250.0|          2022-11-11|                    65| null|      FUJI-FILTER 02|      FUJI-FILTER 03|         null|                  EA|                  120|   0|                     RM|     S001|            D654|                  1|              G1|              C1|       281|                     331|      50|            UC|     3403|         3408|               56|               22|            INR|            97|       1000457-SMT|\n",
      "|  124527|IDS-21E2-00080|     IGS|      IGS|   175.0|          2021-08-27|                   506| null|  RoHS,Security Tape|  RoHS,Security Tape|         null|                  EA|                  120|   0|                     RM|     S006|            D655|                  1|              G1|              C1|       319|                     369|      50|            UC|     3280|         3280|               18|               52|            INR|            85|       1000457-SMT|\n",
      "|  124527|IDS-21E2-00080|     IGS|      IGS|   175.0|          2021-08-27|                   506| null|      FUJI-FILTER 02|RoHSLabel, 3.5 X ...|         null|                  EA|                  120|   0|                     PM|     S009|            D655|                  1|              G1|              C1|       433|                     483|      50|            UC|     3599|         3771|                5|               33|            USD|            97|       1000457-SMT|\n",
      "|  118502|IDS-21E2-00125|     TMG|      TMG|   200.0|          2021-10-14|                   458| null|OTCT-486*377*309M...|OTCT-486*377*309M...|         null|                  EA|                  120|   0|                     RM|     S004|            D655|                  1|              G1|              C1|       691|                     741|      50|            UC|     3220|         3230|               30|               24|            INR|            90|       1000456-SMT|\n",
      "|  118502|IDS-21E2-00125|     TMG|      TMG|   200.0|          2021-10-14|                   458| null|RoHS CARTON,SINGL...|RoHS CARTON,SINGL...|         null|                  EA|                  120|   0|                     RM|     S002|            D668|                  1|              G1|              C1|       678|                     728|      50|            UC|     3428|         3428|               26|               20|            USD|            95|       1000456-SMT|\n",
      "|  118502|IDS-21E2-00125|     TMG|      TMG|   200.0|          2021-10-14|                   458| null|      FUJI-FILTER 02|      FUJI-FILTER 03|         null|                  EA|                  120|   0|                     RM|     S001|            D654|                  1|              G1|              C1|       281|                     331|      50|            UC|     3403|         3408|               56|               22|            INR|            97|       1000457-SMT|\n",
      "|  149104|IDS-21E2-00125|     TMG|      TMG|   200.0|          2021-06-24|                   570| null|  RoHS,Security Tape|  RoHS,Security Tape|         null|                  EA|                  120|   0|                     CN|     S004|            D654|                  1|              G1|              C1|       341|                     391|      50|            UC|     3827|         3029|               26|               50|            INR|            81|       1000457-SMT|\n",
      "|  149104|IDS-21E2-00125|     TMG|      TMG|   200.0|          2021-06-24|                   570| null|INCT-CB-345*293*9...|INCT-CB-345*293*9...|         null|                  EA|                  120|   0|                     PM|     S006|            D668|                  1|              G1|              C1|       389|                     439|      50|            UC|     2833|         2861|               42|               48|            INR|            97|       1000456-SMT|\n",
      "+--------+--------------+--------+---------+--------+--------------------+----------------------+-----+--------------------+--------------------+-------------+--------------------+---------------------+----+-----------------------+---------+----------------+-------------------+----------------+----------------+----------+------------------------+--------+--------------+---------+-------------+-----------------+-----------------+---------------+--------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "step4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcadce57",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/c:/Users/Nasim/Desktop/mishal/nest-data-hackathon/6_SERVING_LAYER/LAKEHOUSE_GOLD_LAYER/scfinal_step4.csv already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m step4\u001b[39m.\u001b[39;49mcoalesce(\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mformat(\u001b[39m'\u001b[39;49m\u001b[39mcom.databricks.spark.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49moptions(header\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrue\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49msave(\u001b[39m'\u001b[39;49m\u001b[39m../6_SERVING_LAYER/LAKEHOUSE_GOLD_LAYER/scfinal_step4.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Nasim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:968\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jwrite\u001b[39m.\u001b[39msave()\n\u001b[0;32m    967\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49msave(path)\n",
      "File \u001b[1;32mc:\\Users\\Nasim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Users\\Nasim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: path file:/c:/Users/Nasim/Desktop/mishal/nest-data-hackathon/6_SERVING_LAYER/LAKEHOUSE_GOLD_LAYER/scfinal_step4.csv already exists."
     ]
    }
   ],
   "source": [
    "step4.coalesce(1).write.format('com.databricks.spark.csv').options(header='true').save('../6_SERVING_LAYER/LAKEHOUSE_GOLD_LAYER/scfinal_step4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e7e4c237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|Bin Code|    Quantity|\n",
      "+--------+------------+\n",
      "|   TMG.1|     18900.0|\n",
      "|     TMG| 3.5393905E8|\n",
      "|     IGS|2.61391425E8|\n",
      "+--------+------------+\n",
      "\n",
      "+---------------+--------------+--------+--------+\n",
      "|       Item No_|       Lot No_|Bin Code|Quantity|\n",
      "+---------------+--------------+--------+--------+\n",
      "|        1000457|IDS-21E2-00416|     IGS|136750.0|\n",
      "|         108259|IDS-21E2-00325|     TMG|274050.0|\n",
      "|         108259|IDS-21E2-00628|     IGS|430650.0|\n",
      "|         110951|IDS-21E2-00368|     TMG|   400.0|\n",
      "|         115132|IDS-21E2-00125|     TMG|  1400.0|\n",
      "|         117232|IDS-21E2-00056|     TMG|  6475.0|\n",
      "|         122106|IDS-21E2-00039|     IGS|  2975.0|\n",
      "|         124257|IDS-21E2-00560|     IGS|  2275.0|\n",
      "|         137932|IDS-21E2-00372|     TMG|  9100.0|\n",
      "|         138531|IDS-21E2-00724|     TMG| 77750.0|\n",
      "|         141388|IDS-21E2-00125|     IGS|  4500.0|\n",
      "|         145340|IDS-21E2-00876|     TMG| 51000.0|\n",
      "|       700536-2|IDS-21E2-00837|     IGS|  6825.0|\n",
      "|EII-CNS-OT-0007|IDS-21E2-00125|     TMG|  1050.0|\n",
      "|EII-CNS-OT-0030|IDS-21E2-00568|     IGS|  1600.0|\n",
      "|        1000457|IDS-21E2-00912|     IGS|109400.0|\n",
      "|        1000457|IDS-21E2-00328|     IGS|246150.0|\n",
      "|         102565|IDS-21E2-00432|     IGS| 39375.0|\n",
      "|         102565|IDS-21E2-01074|     TMG| 56250.0|\n",
      "|         102565|IDS-21E2-00083|     TMG| 95625.0|\n",
      "+---------------+--------------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------+--------------+--------+-----------+\n",
      "|Item No_|       Lot No_|Bin Code|   Quantity|\n",
      "+--------+--------------+--------+-----------+\n",
      "|  119852|IDS-21E2-00039|     TMG| 1.717745E7|\n",
      "|  119852|IDS-21E2-00039|     IGS|  1.69511E7|\n",
      "|  119852|IDS-21E2-00125|     TMG| 1.531635E7|\n",
      "|  108259|IDS-21E2-00039|     TMG|1.3173975E7|\n",
      "|  108259|IDS-21E2-00039|     IGS|  1.28412E7|\n",
      "|  119852|IDS-21E2-00125|     IGS| 1.154385E7|\n",
      "|  119852|IDS-21E2-00560|     TMG|  9707900.0|\n",
      "|  119852|IDS-21E2-00564|     TMG|  8576150.0|\n",
      "|  108259|IDS-21E2-00560|     TMG|  8534700.0|\n",
      "|  119852|IDS-21E2-00560|     IGS|  8374950.0|\n",
      "+--------+--------------+--------+-----------+\n",
      "\n",
      "+---------------+--------------+--------+----------------------+\n",
      "|       Item No_|       Lot No_|Bin Code|Days from Registration|\n",
      "+---------------+--------------+--------+----------------------+\n",
      "|         111742|IDS-21E2-00820|     TMG|                   528|\n",
      "|         112048|IDS-21E2-00816|     TMG|                   702|\n",
      "|         114717|IDS-21E2-00101|     IGS|                   690|\n",
      "|         119396|IDS-21E2-01072|     IGS|                   383|\n",
      "|         120958|IDS-21E2-00596|     TMG|                   272|\n",
      "|         131536|IDS-21E2-00039|     IGS|                   532|\n",
      "|         139272|IDS-21E2-00086|     IGS|                   660|\n",
      "|         145340|IDS-21E2-00372|     IGS|                   649|\n",
      "|         145340|IDS-21E2-00039|     TMG|                   700|\n",
      "|         145433|IDS-21E2-00564|     TMG|                   387|\n",
      "|EII-CNS-OT-0008|IDS-21E2-00114|     TMG|                   405|\n",
      "|        1000457|IDS-21E2-00756|     TMG|                   704|\n",
      "|        1000457|IDS-21E2-00432|     TMG|                   648|\n",
      "|         102565|IDS-21E2-00574|     TMG|                   189|\n",
      "|         103557|IDS-21E2-00054|     TMG|                   392|\n",
      "|         105449|IDS-21E2-00332|     TMG|                   315|\n",
      "|         105695|IDS-21E2-00086|     IGS|                   341|\n",
      "|         108259|IDS-21E2-00563|     TMG|                   176|\n",
      "|         111434|IDS-21E2-00624|     IGS|                   492|\n",
      "|         112296|IDS-21E2-01012|     IGS|                   239|\n",
      "+---------------+--------------+--------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "3941 10 3941 3\n"
     ]
    }
   ],
   "source": [
    "step4.createOrReplaceTempView(\"step4table\")\n",
    "step4table = spark.table(\"step4table\")\n",
    "newdf1 = step4table.groupBy([\"Item No_\",\"Lot No_\",\"Bin Code\"]).agg(sum(\"Quantity\").alias(\"Quantity\")).distinct()\n",
    "newdf2 = step4table.groupBy([\"Item No_\",\"Lot No_\",\"Bin Code\"]).agg(sum(\"Quantity\").alias(\"Quantity\")).sort(desc(\"Quantity\")).distinct().limit(10)\n",
    "newdf3 = step4.select(\"Item No_\",\"Lot No_\",\"Bin Code\", \"Days from Registration\").sort(desc(\"Days from Registration\")).distinct()\n",
    "newdf4 = step4table.groupBy(\"Bin Code\").agg(sum(\"Quantity\").alias(\"Quantity\")).dropna().distinct()\n",
    "#print(newdf4.count())\n",
    "#print(newdf4.count())\n",
    "newdf4.show()\n",
    "newdf1.show()\n",
    "newdf2.show()\n",
    "newdf3.show()\n",
    "print(newdf1.count(),newdf2.count(),newdf3.count(),newdf4.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d262d026",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< Updated upstream
   "display_name": "Python 3.11.1 64-bit",
=======
   "display_name": "venv",
>>>>>>> Stashed changes
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "74f652d1691aae558687d11d473e460985fd78a87377b917f434b5d3c133506a"
=======
   "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "80f38e7ec0327a6414a55918c31a015cabb3c453423c70751535812149cec50d"
>>>>>>> Stashed changes
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
