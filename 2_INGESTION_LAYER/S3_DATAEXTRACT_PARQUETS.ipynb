{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2675f785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import io\n",
    "\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d6911a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client =boto3.client('s3')\n",
    "s3_bucket_name='hackathon2023'\n",
    "s3 = boto3.resource('s3',\n",
    "                    aws_access_key_id= 'AKIA3AEXDSNEGXQERCGG',\n",
    "                    aws_secret_access_key='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d16e803a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3.Bucket.objectsCollection(s3.Bucket(name='hackathon2023'), s3.ObjectSummary)\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (RequestTimeTooSkewed) when calling the ListObjects operation: The difference between the request time and the current time is too large.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(my_bucket\u001b[39m.\u001b[39mobjects\u001b[39m.\u001b[39mfilter(Prefix\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      3\u001b[0m bucket_list \u001b[39m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m my_bucket\u001b[39m.\u001b[39mobjects\u001b[39m.\u001b[39mfilter(Prefix \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m      6\u001b[0m     file_name\u001b[39m=\u001b[39mfile\u001b[39m.\u001b[39mkey\n\u001b[0;32m      7\u001b[0m     \u001b[39mif\u001b[39;00m file_name\u001b[39m.\u001b[39mfind(\u001b[39m\"\u001b[39m\u001b[39m.parquet\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m!=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n",
      "File \u001b[1;32md:\\NEST_DATALAKEHOUSE\\venv\\lib\\site-packages\\boto3\\resources\\collection.py:81\u001b[0m, in \u001b[0;36mResourceCollection.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m limit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mlimit\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     80\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m---> 81\u001b[0m \u001b[39mfor\u001b[39;00m page \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpages():\n\u001b[0;32m     82\u001b[0m     \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m page:\n\u001b[0;32m     83\u001b[0m         \u001b[39myield\u001b[39;00m item\n",
      "File \u001b[1;32md:\\NEST_DATALAKEHOUSE\\venv\\lib\\site-packages\\boto3\\resources\\collection.py:171\u001b[0m, in \u001b[0;36mResourceCollection.pages\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39m# Now that we have a page iterator or single page of results\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39m# we start processing and yielding individual items.\u001b[39;00m\n\u001b[0;32m    170\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 171\u001b[0m \u001b[39mfor\u001b[39;00m page \u001b[39min\u001b[39;00m pages:\n\u001b[0;32m    172\u001b[0m     page_items \u001b[39m=\u001b[39m []\n\u001b[0;32m    173\u001b[0m     \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handler(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent, params, page):\n",
      "File \u001b[1;32md:\\NEST_DATALAKEHOUSE\\venv\\lib\\site-packages\\botocore\\paginate.py:269\u001b[0m, in \u001b[0;36mPageIterator.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inject_starting_params(current_kwargs)\n\u001b[0;32m    268\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_request(current_kwargs)\n\u001b[0;32m    270\u001b[0m     parsed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extract_parsed_response(response)\n\u001b[0;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m first_request:\n\u001b[0;32m    272\u001b[0m         \u001b[39m# The first request is handled differently.  We could\u001b[39;00m\n\u001b[0;32m    273\u001b[0m         \u001b[39m# possibly have a resume/starting token that tells us where\u001b[39;00m\n\u001b[0;32m    274\u001b[0m         \u001b[39m# to index into the retrieved page.\u001b[39;00m\n",
      "File \u001b[1;32md:\\NEST_DATALAKEHOUSE\\venv\\lib\\site-packages\\botocore\\paginate.py:357\u001b[0m, in \u001b[0;36mPageIterator._make_request\u001b[1;34m(self, current_kwargs)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_make_request\u001b[39m(\u001b[39mself\u001b[39m, current_kwargs):\n\u001b[1;32m--> 357\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_method(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcurrent_kwargs)\n",
      "File \u001b[1;32md:\\NEST_DATALAKEHOUSE\\venv\\lib\\site-packages\\botocore\\client.py:530\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    527\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpy_operation_name\u001b[39m}\u001b[39;00m\u001b[39m() only accepts keyword arguments.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    528\u001b[0m     )\n\u001b[0;32m    529\u001b[0m \u001b[39m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[1;32m--> 530\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[1;32md:\\NEST_DATALAKEHOUSE\\venv\\lib\\site-packages\\botocore\\client.py:960\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m    958\u001b[0m     error_code \u001b[39m=\u001b[39m parsed_response\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mError\u001b[39m\u001b[39m\"\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mCode\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    959\u001b[0m     error_class \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mfrom_code(error_code)\n\u001b[1;32m--> 960\u001b[0m     \u001b[39mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[0;32m    961\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    962\u001b[0m     \u001b[39mreturn\u001b[39;00m parsed_response\n",
      "\u001b[1;31mClientError\u001b[0m: An error occurred (RequestTimeTooSkewed) when calling the ListObjects operation: The difference between the request time and the current time is too large."
     ]
    }
   ],
   "source": [
    "my_bucket=s3.Bucket(s3_bucket_name)\n",
    "print(my_bucket.objects.filter(Prefix='data'))\n",
    "bucket_list = []\n",
    "for file in my_bucket.objects.filter(Prefix = 'data'):\n",
    "    \n",
    "    file_name=file.key\n",
    "    if file_name.find(\".parquet\")!=-1:\n",
    "        bucket_list.append(file.key)\n",
    "length_bucket_list=print(len(bucket_list))\n",
    "print(bucket_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1095c3d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/10 23:59:06 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://hackathon2023/data/OperationsManagement/RoutingStages/RoutingStages.parquet.\n",
      "java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n",
      "\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)\n",
      "\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n",
      "\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n",
      "\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:562)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n",
      "\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n",
      "\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n",
      "\t... 26 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o43.parquet.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:562)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 29 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3a://hackathon2023/data/OperationsManagement/RoutingStages/RoutingStages.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/sql/readwriter.py:364\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    353\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    355\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    356\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    362\u001b[0m )\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o43.parquet.\n: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688)\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3431)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:752)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:750)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:579)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:408)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:210)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:562)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2592)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2686)\n\t... 29 more\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"s3a://hackathon2023/data/OperationsManagement/RoutingStages/RoutingStages.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8f6a5fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m routingdata \u001b[38;5;241m=\u001b[39m  s3\u001b[38;5;241m.\u001b[39mObject(s3_bucket_name,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/OperationsManagement/RoutingStages/RoutingStages.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mget()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      2\u001b[0m routingnames \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mId | BatchId | WorkOrderId | PlacementOpportunities | TerminationOpportunities | AssemblyOpportunities | ComponentOpportunities | UpperLimit | DisplayOrder | OperationNo | NextOperationNo | PreviousOperationNo | Name | EstimatedTime | OnlineInspection | ICTStage | ProcessCheck | QAPercentage | HasSA | saValidationMethod | Surface | InspectionMode | TrayReleasingStage | TimeLineStartStage | TimeLineActionMode | TimeLineActionInstance | AutoRejectLimit | ReRoutingActionStage | OfflineAction | OfflineProcessDuration | TimeLineDuration | PCBStarting | CleaningDelay | ReRoutingStage | AOIStage | IsMachineTraceabilityRequired | QaAfterRework | CustomerSerialNumberCheck | UsrLckOnError | IsOptionalStage | QASamplingMethod | QASamplingValueType | NoOfAllowedFailure | ReRoutingStageOnReWork | ReRoutingMethod | IsCheckListRequired | OutputQuantity | StartTime | EndTime | IdleTime | DelayTime | Started | EcnStage | PostedQuantity | StageStartTime | HaveDeviation | IsXmlExportRequired | XmlFilePath | IsWCQualifyReq | IsImeiNumberRequired | MSDDetails_Id | ReworkStationOnRework | ReworkStationDefect | ReworkOrDebugLimit | EDIReportRequired | EDIReportFormat | RecleaningAcceptLimit | Company | Division\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m routing_table \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39mread_table(io\u001b[38;5;241m.\u001b[39mBytesIO(routingdata))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "routingdata =  s3.Object(s3_bucket_name,'data/OperationsManagement/RoutingStages/RoutingStages.parquet').get()['Body'].read()\n",
    "routingnames = \"Id | BatchId | WorkOrderId | PlacementOpportunities | TerminationOpportunities | AssemblyOpportunities | ComponentOpportunities | UpperLimit | DisplayOrder | OperationNo | NextOperationNo | PreviousOperationNo | Name | EstimatedTime | OnlineInspection | ICTStage | ProcessCheck | QAPercentage | HasSA | saValidationMethod | Surface | InspectionMode | TrayReleasingStage | TimeLineStartStage | TimeLineActionMode | TimeLineActionInstance | AutoRejectLimit | ReRoutingActionStage | OfflineAction | OfflineProcessDuration | TimeLineDuration | PCBStarting | CleaningDelay | ReRoutingStage | AOIStage | IsMachineTraceabilityRequired | QaAfterRework | CustomerSerialNumberCheck | UsrLckOnError | IsOptionalStage | QASamplingMethod | QASamplingValueType | NoOfAllowedFailure | ReRoutingStageOnReWork | ReRoutingMethod | IsCheckListRequired | OutputQuantity | StartTime | EndTime | IdleTime | DelayTime | Started | EcnStage | PostedQuantity | StageStartTime | HaveDeviation | IsXmlExportRequired | XmlFilePath | IsWCQualifyReq | IsImeiNumberRequired | MSDDetails_Id | ReworkStationOnRework | ReworkStationDefect | ReworkOrDebugLimit | EDIReportRequired | EDIReportFormat | RecleaningAcceptLimit | Company | Division\".split(\" | \")\n",
    "routing_table = spark.read_table(io.BytesIO(routingdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705303a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          id                                    BatchId  \\\n",
      "0       W-749816B-8722848856   W-749816-Items-EE-07197264-UK-4746307309   \n",
      "1      W-749816B-71801103384  W-749816-Items-EE-07197264-UK-60187728422   \n",
      "2      W-749816B-48685952117  W-749816-Items-EE-07197264-UK-31849426921   \n",
      "3      W-749816B-25850813917  W-749816-Items-EE-07197264-UK-11178545403   \n",
      "4       W-749816B-5596928600  W-749816-Items-EE-07197264-UK-22025055861   \n",
      "...                      ...                                        ...   \n",
      "77137  W-989803B-34904579707  W-989803-Items-EP-07767398-CH-85834879465   \n",
      "77138  W-989803B-89923620898  W-989803-Items-EP-07767398-CH-16619212410   \n",
      "77139  W-989803B-91771370800  W-989803-Items-EP-07767398-CH-54213582195   \n",
      "77140  W-989803B-58782909661  W-989803-Items-EP-07767398-CH-61456024033   \n",
      "77141  W-989803B-28992178339  W-989803-Items-EP-07767398-CH-84254740039   \n",
      "\n",
      "      WorkOrderId  PlacementOpportunities  TerminationOpportunities  \\\n",
      "0        W-749816                       0                         0   \n",
      "1        W-749816                       0                         0   \n",
      "2        W-749816                       0                         0   \n",
      "3        W-749816                       0                         0   \n",
      "4        W-749816                       0                         0   \n",
      "...           ...                     ...                       ...   \n",
      "77137    W-989803                       0                         0   \n",
      "77138    W-989803                       0                         0   \n",
      "77139    W-989803                       0                         0   \n",
      "77140    W-989803                       0                         0   \n",
      "77141    W-989803                       0                         0   \n",
      "\n",
      "       AssemblyOpportunities  ComponentOpportunities  UpperLimit  \\\n",
      "0                          0                       0           0   \n",
      "1                          0                       0           0   \n",
      "2                          0                       0           0   \n",
      "3                          1                       0           0   \n",
      "4                          1                       0           0   \n",
      "...                      ...                     ...         ...   \n",
      "77137                      1                       0           0   \n",
      "77138                      1                       0           0   \n",
      "77139                      0                       0           0   \n",
      "77140                      0                       0           0   \n",
      "77141                      0                       0           0   \n",
      "\n",
      "       DisplayOrder  OperationNo  ...  IsImeiNumberRequired  MSDDetails_Id  \\\n",
      "0                 0            0  ...                     0            NaN   \n",
      "1                 1            1  ...                     0            NaN   \n",
      "2                 2            2  ...                     0            NaN   \n",
      "3                 3            3  ...                     0            NaN   \n",
      "4                 4            4  ...                     0            NaN   \n",
      "...             ...          ...  ...                   ...            ...   \n",
      "77137            87           87  ...                     0            NaN   \n",
      "77138            88           88  ...                     0            NaN   \n",
      "77139            89           89  ...                     0            NaN   \n",
      "77140            90           90  ...                     0            NaN   \n",
      "77141            91           91  ...                     0            NaN   \n",
      "\n",
      "      ReworkStationOnRework  ReworkStationDefect  ReworkOrDebugLimit  \\\n",
      "0                         0                  NaN                   0   \n",
      "1                         0                  NaN                   0   \n",
      "2                         0                  NaN                   0   \n",
      "3                         0                  NaN                   0   \n",
      "4                         0                  NaN                   0   \n",
      "...                     ...                  ...                 ...   \n",
      "77137                     0                  NaN                   0   \n",
      "77138                     0                  NaN                   0   \n",
      "77139                     0                  NaN                   0   \n",
      "77140                     0                  NaN                   0   \n",
      "77141                     0                  NaN                   0   \n",
      "\n",
      "       EDIReportRequired  EDIReportFormat  RecleaningAcceptLimit  Company  \\\n",
      "0                      0                0                    NaN            \n",
      "1                      0                0                    NaN            \n",
      "2                      0                0                    NaN            \n",
      "3                      0                0                    NaN            \n",
      "4                      0                0                    NaN            \n",
      "...                  ...              ...                    ...      ...   \n",
      "77137                  0                0                    NaN            \n",
      "77138                  0                0                    NaN            \n",
      "77139                  0                0                    NaN            \n",
      "77140                  0                0                    NaN            \n",
      "77141                  0                0                    NaN            \n",
      "\n",
      "       Division  \n",
      "0                \n",
      "1                \n",
      "2                \n",
      "3                \n",
      "4                \n",
      "...         ...  \n",
      "77137            \n",
      "77138            \n",
      "77139            \n",
      "77140            \n",
      "77141            \n",
      "\n",
      "[77142 rows x 69 columns]\n"
     ]
    }
   ],
   "source": [
    "routingdata =  s3.Object(s3_bucket_name,'data/OperationsManagement/RoutingStages/RoutingStages.parquet').get()['Body'].read()\n",
    "routingnames = \"Id | BatchId | WorkOrderId | PlacementOpportunities | TerminationOpportunities | AssemblyOpportunities | ComponentOpportunities | UpperLimit | DisplayOrder | OperationNo | NextOperationNo | PreviousOperationNo | Name | EstimatedTime | OnlineInspection | ICTStage | ProcessCheck | QAPercentage | HasSA | saValidationMethod | Surface | InspectionMode | TrayReleasingStage | TimeLineStartStage | TimeLineActionMode | TimeLineActionInstance | AutoRejectLimit | ReRoutingActionStage | OfflineAction | OfflineProcessDuration | TimeLineDuration | PCBStarting | CleaningDelay | ReRoutingStage | AOIStage | IsMachineTraceabilityRequired | QaAfterRework | CustomerSerialNumberCheck | UsrLckOnError | IsOptionalStage | QASamplingMethod | QASamplingValueType | NoOfAllowedFailure | ReRoutingStageOnReWork | ReRoutingMethod | IsCheckListRequired | OutputQuantity | StartTime | EndTime | IdleTime | DelayTime | Started | EcnStage | PostedQuantity | StageStartTime | HaveDeviation | IsXmlExportRequired | XmlFilePath | IsWCQualifyReq | IsImeiNumberRequired | MSDDetails_Id | ReworkStationOnRework | ReworkStationDefect | ReworkOrDebugLimit | EDIReportRequired | EDIReportFormat | RecleaningAcceptLimit | Company | Division\".split(\" | \")\n",
    "routing_table = pq.read_table(io.BytesIO(routingdata))\n",
    "routingdf = routing_table.to_pandas()\n",
    "\n",
    "routingdf.to_csv('../LAKEHOUSE_SILVER_LAYER/routing1.csv', header=True, index=False)\n",
    "\n",
    "print(routingdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06225d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         No_ No_ 2                                       Description  \\\n",
      "1      135677  None              STATIC SHIELDED BAGS OF SIZE 280x220   \n",
      "2      119852  None              STATIC SHIELDED BAGS OF SIZE 280x220   \n",
      "3      147831  None                              RoHS,Label, 4X6 inch   \n",
      "4      105695  None  RoHSLabel, 3.5 X 1 inch blank (outer box labels/   \n",
      "5      141663  None                                RoHS,Security Tape   \n",
      "...       ...   ...                                               ...   \n",
      "11996  119852  None       RoHS,LABEL-CRUCIAL LABEL--*-54*12.8MM-GLOSS   \n",
      "11997  147359  None                     RoHS PCB BOARD, IQ CONTROLLER   \n",
      "11998  112048  None                                            _RARE_   \n",
      "11999  119852  None                  OTCT-377*192*309MM-200PCS DIMM-Y   \n",
      "12000  105695  None                                       RoHS,DRAM-1   \n",
      "\n",
      "0                                    Search Description Description 2  \\\n",
      "1                  STATIC SHIELDED BAGS OF SIZE 280x221          None   \n",
      "2                  STATIC SHIELDED BAGS OF SIZE 280x221          None   \n",
      "3                                  RoHS,Label, 4X6 inch          None   \n",
      "4      RoHSLabel, 3.5 X 1 inch blank (outer box labels/          None   \n",
      "5                                    RoHS,Security Tape          None   \n",
      "...                                                 ...           ...   \n",
      "11996       RoHS,LABEL-CRUCIAL LABEL--*-54*12.8MM-GLOSS          None   \n",
      "11997                     RoHS PCB BOARD, IQ CONTROLLER          None   \n",
      "11998                                            _RARE_          None   \n",
      "11999                  OTCT-377*192*309MM-200PCS DIMM-Y          None   \n",
      "12000                                RoHS,Security Tape          None   \n",
      "\n",
      "0     Base Unit of Measure Price Unit Conversion Type Inventory Posting Group  \\\n",
      "1                       EA                   120    0                      CN   \n",
      "2                       EA                   120    0                      RM   \n",
      "3                       EA                   120    0                      PM   \n",
      "4                       EA                   120    0                      PM   \n",
      "5                       EA                   120    0                      RM   \n",
      "...                    ...                   ...  ...                     ...   \n",
      "11996                   EA                   120    0                      PM   \n",
      "11997                   EA                   120    0                      CN   \n",
      "11998                   EA                   120    0                      CN   \n",
      "11999                   EA                   120    0                      CN   \n",
      "12000                   EA                   120    0                      PM   \n",
      "\n",
      "0     Shelf No_  ... Price_Profit Calculation Profit _ Costing Method  \\\n",
      "1          S001  ...                      536       50             UC   \n",
      "2          S001  ...                      301       50             UC   \n",
      "3          S003  ...                      314       50             UC   \n",
      "4          S007  ...                      468       50             UC   \n",
      "5          S001  ...                      347       50             UC   \n",
      "...         ...  ...                      ...      ...            ...   \n",
      "11996      S004  ...                      660       50             UC   \n",
      "11997      S002  ...                      564       50             UC   \n",
      "11998      S002  ...                      785       50             UC   \n",
      "11999      S006  ...                      453       50             UC   \n",
      "12000      S006  ...                      313       50             UC   \n",
      "\n",
      "0     Unit Cost Standard Cost Quoted Price(INR) Quoted Price(FCY)  \\\n",
      "1          2541          2554                31                26   \n",
      "2          3799          3799                16                49   \n",
      "3          3772          3872                50                 6   \n",
      "4          2807          2861                29                 9   \n",
      "5          2647          3657                60                 4   \n",
      "...         ...           ...               ...               ...   \n",
      "11996      2936          2962                36                 5   \n",
      "11997      3734          3227                54                54   \n",
      "11998      2550          2558                27                 3   \n",
      "11999      2763          2769                22                37   \n",
      "12000      3257          3257                31                 1   \n",
      "\n",
      "0     Quoted Currency Standard Cost_ Production BOM No_  \n",
      "1                 USD             86        1000456-SMT  \n",
      "2                 USD             86        1000456-SMT  \n",
      "3                 USD             82        1000456-SMT  \n",
      "4                 INR             88        1000456-SMT  \n",
      "5                 USD             75               None  \n",
      "...               ...            ...                ...  \n",
      "11996             USD             86        1000456-SMT  \n",
      "11997             USD             95               None  \n",
      "11998             INR            100        1000457-SMT  \n",
      "11999             USD             98        1000457-SMT  \n",
      "12000             USD             78        1000457-SMT  \n",
      "\n",
      "[12000 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "itemdata =  s3.Object(s3_bucket_name,'data/SCSupplyChain/item/item.parquet').get()['Body'].read()\n",
    "itemnames = \"No_ | No_ 2 | Description | Search Description | Description 2 | Base Unit of Measure | Price Unit Conversion | Type | Inventory Posting Group | Shelf No_ | Item Disc_ Group | Allow Invoice Disc_ | Statistics Group | Commission Group | Unit Price | Price_Profit Calculation | Profit _ | Costing Method | Unit Cost | Standard Cost | Quoted Price(INR) | Quoted Price(FCY) | Quoted Currency | Standard Cost_ | Production BOM No_\".split(\" | \")\n",
    "item_table = pq.read_table(io.BytesIO(itemdata))\n",
    "itemdf = item_table.to_pandas()\n",
    "\n",
    "itemdf.loc[-1] = itemnames\n",
    "\n",
    "# Reset the index so that the new row is at the top\n",
    "itemdf.index = itemdf.index + 1\n",
    "itemdf = itemdf.sort_index()\n",
    "\n",
    "# Set the header row as the new column names\n",
    "itemdf.columns = itemdf.iloc[0]\n",
    "\n",
    "# Remove the header row from the DataFrame\n",
    "itemdf = itemdf.iloc[1:]\n",
    "\n",
    "itemdf.to_csv('../LAKEHOUSE_SILVER_LAYER/items1.csv', header=True, index=False)\n",
    "\n",
    "print(itemdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3290d17b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "80f38e7ec0327a6414a55918c31a015cabb3c453423c70751535812149cec50d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
